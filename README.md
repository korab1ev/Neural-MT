# To-do:
1. __Investigate why vanilla enc-dec architecture bias towards shorter candidates__
2. __Attention mode = 'Dot-product','Bilinear','Multi-Layer Perceptron'__
3. __class LuongModel__
4. __class BagdanauModel__
5. __Make beam research on these two models__
6. __Try bidirectional encoder__
7. __Visualise attentions__
8. __Try reversing the input sequence in the decoder__(vanilla enc-dec architecture, try to find that article)
