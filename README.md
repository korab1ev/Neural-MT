# To-do:
1. __Do length normalization__ (the model is bias towards shorter candidates)
2. __Attention mode = 'Dot-product','Bilinear','Multi-Layer Perceptron'__
3. __class LuongModel__
4. __class BagdanauModel__
5. __Make beam research on these two models__
6. __Try bidirectional encoder__
7. __Visualise attentions__
8. __Try reversing the input sequence in the decoder__(vanilla enc-dec architecture, try to find that article)
